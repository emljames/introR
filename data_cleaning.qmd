---
title: "Data cleaning in R"
output: 
  html_document:
    toc: true
    toc_depth: 1
---

In this section of the workshop, we will learn how to manipulate your data in R. Being able to process and reformat your data in a fast and reproducible way is a major benefit of working in R relative to e.g., Excel and SPSS. We will use a set of functions dedicated to this purpose, known as the *tidyverse*. In this section, we will refine our dataset based on the variables and observations that we actually need.

# Setting up

## Load packages

*(If you're following this material directly after the [Navigating R](https://emljames.github.io/introR/navigating_R.html) content, then you've already done this first step!)*

We need to load a collection of functions from the *tidyverse*. If you have not already installed this package, you still need to run `install.packages("tidyverse")` in the Console first.

```{r libraries, message = FALSE, warning = FALSE}
library(tidyverse)
```

## Load data

We will be working with data collected from a study of sleep and word learning in children ([James, Gaskell, & Henderson, 2020](https://acamh.onlinelibrary.wiley.com/doi/full/10.1111/jcpp.13253)). The experiment involved teaching children the names of unusual plants/animals, and testing their memory for them after periods of wake and sleep. For this lesson, we will analyse the data from the picture naming task, in which children were asked to name pictures of the items as quickly as they could. We will focus on data from the first two test sessions only (12-hour period containing wake or sleep).

The folder circulated included folder with the data in it, so we need to instruct R to navigate to the correct file within the raw data folder. If you look in the files pane, you should see that we are currently located in the scripts folder. To navigate to the data folder, we need to include "../" in the file path to instruct R to go "up one level", then "data/" to go into the data folder, "raw/" to go into the raw data folder, then the file name (including path extension).

Let's read in the picture naming file, and assign it to the object "raw_dat". Remember, the `<-` means that the action to the right is assigned to the "object" name on the left.

```{r load-data, eval = FALSE}
raw_dat <- read_csv("../data/raw/picName_raw.csv")
```

```{r load-data-orig, include=FALSE}
raw_dat <- read_csv("./workshop_files/data/raw/picName_raw.csv")
```

You should now see an object named `raw_dat` in the environment pane, on the top right of your RStudio window. You can see that it has `r nrow(raw_dat)` observations (rows) for `r ncol(raw_dat)` variables (columns). You can inspect this by click on the little grid icon next to it.

We can also inspect the data format by calling `summary()` to preview the data, and using the `head()` function to print the first few rows. Run these lines of code and inspect the output.

```{r preview-dat, eval = FALSE}
# Print summary of variables
summary(raw_dat)

# Print first rows of dataset 
head(raw_dat)
```

You don't need to worry too much about what these different variables mean for the purposes of this workshop, but you can see here that this is trial level data. The dependent variables are accuracy and RT, and we have these across several items for each participant. A full data dictionary is described below.

## Data dictionary

-   **ID** --- a unique anonymised code for each participant
-   **group** --- whether participants were good comprehenders (GC) or poor comprehenders (PC)
-   **task** --- which task this data comes from (all PNN)
-   **week** --- whether participants completed this learnTime condition in the first or second week (counterbalanced)
-   **learnTime** --- whether participants learned the items in the morning (AM) or evening (PM). This was a repeated measures manipulation---all participants completed both a morning and evening condition to compare sleep and wake effects on memory.
-   **time** --- whether the data are from test 1 (immediately after learning), test 2 (12 hours later), test 3 (24 hours later), or test 4 (one month later)
-   **item** --- the word trained and tested
-   **acc** --- whether the participant successfully recalled the word in the picture naming task
-   **RT** --- response time (ms), for accurate responses only

We don't necessarily need everything here, so let's start wrangling!

# `select()` relevant variables

Often our means of collecting data give us more information than we really care about. For example, Gorilla outputs all sorts of columns for screen size, browser etc. that won't be helpful for our analysis. We likely want to create a refined version of the dataset that includes on the variables of interest.

To do this, we can use the `select()` function. Let's say for example that we only need the variables *ID, item* and *acc*. We can do this as follows:

```{r select-var1}
# Select by name
select_dat1 <- select(.data = raw_dat, ID, item, acc)
```

In this code chunk, we create a new dataframe called `select_dat1`. The `select()` function takes the name of the dataframe you want to select from as the first argument (in this case, `raw_dat`). We then pass it the names of the variables we need. You should now see this dataframe in the environment pane - we still have all `r nrow(select_dat1)` observations, but only the `r ncol(select_dat1)` variables we've specified. You can click the grid to see this for yourself.

If you have lots of variables that you want to keep, you might want a quicker way of selecting lots of variables at once. Try running these two alternatives:

```{r select-var2}
# Select sequential columns by name
select_dat2 <- select(.data = raw_dat, ID, item:RT)

# Select columns by column number
select_dat3 <- select(.data = raw_dat, 1, 5:7, 9)
```

Compare the new dataframes (*select_dat2*, *select_dat3*) to your original one. What is the `:` doing? How have the numbers worked to select columns in the second example?

Finally, it might sometimes be quicker just to specify the variables you *don't* need. You can do this by using a `-` sign in front of the variable name. For example, the task column is redundant here as we only have data from one task, so we can remove it as follows:

```{r select-var3}
# Drop columns 
select_dat4 <- select(.data = raw_dat, -task)
```

We've seen here some different ways that you can tidy your dataset for relevant variables that you need. Importantly, we've still kept our *original* dataframe (*raw_dat*), so we can always edit and re-run this code if there's something we've missed - be that by mistake as we go about writing our scripts, or further down the line when a reviewer asks for an obscure piece of information you didn't think was relevant. Tidying our data in this way is fast and efficient, and avoids problems caused by trying to retrieve data and match it across raw and processed datasets.

Before we move on, let's use your new column-selecting skills.

::: {.callout-tip title="EXERCISE 1"}
**EXERCISE 1:** Create a new data frame called `selected_vars` which includes the following six variables: ID, learnTime, time, item, acc, RT
:::

```{r exercise1-solution}
#| code-fold: true
#| code-summary: "Show solution"
selected_vars <- select(.data = raw_dat, ID, learnTime, time, item, acc, RT)
```

# `rename() variables`

Once your dataset is a more manageable size, you might find that you want to rename your key variables. For example, in our current dataset, the difference between "learnTime" and "time" is not intuitive. Let's rename these variables to make the difference clearer (and thereby reduce the risk of making mistakes in our code!).

You can do this using the `rename()` function, following the format `new_name = old_name`. Let's do this with our raw dataset, renaming just the learnTime and time columns to better distinguish them:

```{r rename1}
rename_dat1 <- rename(.data = raw_dat, encode_time = learnTime, test_time = time)
```

As with the other *tidyverse* functions, the name of the dataset is needed as the first argument, then we simply list the new variable names that we want assigned to our old variable names. If you inspect the new dataframe (`rename_dat1`), you should be able to see that the two variables now have different names to before.

There are a few rules for creating column names! The main one is that names cannot include spaces. They can include letters (case sensitive), basic punctuation (dashes, underscores, points), and numbers as long as they are not at the start of the name. It's a good idea to adopt a consistent style for tidiness and your own memory.

::: {.callout-warning title="Poorly formatted column names"}
If your loaded data file \*does\* include spaces in column names, you will need to surround them in back-ticks to use them until you rename them.
:::

![[Artwork by \@allison_horst](https://x.com/allison_horst)](images/common_cases.png){fig-alt="Cartoon representations of common cases in coding. A snake screams \"SCREAMING_SNAKE_CASE\" into the face of a camel (wearing ear muffs) with \"camelCase\" written along its back. Vegetables on a skewer spell out \"kebab-case\" (words on a skewer). A mellow, happy looking snake has text \"snake_case\" along it."}

::: {.callout-tip title="EXERCISE 2"}
Create a new dataframe called renamed_vars, by renaming your selected_vars dataframe from Exercise 1. Edit the column names for the following variables:

-   ID \>\> ppt_ID
-   learnTime \>\> learn_time
-   time \>\> test_time
:::

```{r exercise2-solution}
#| code-fold: true 
#| code-summary: "Show solution"  
renamed_vars <- selected_vars |> 
  rename(ppt_ID = ID, learn_time = learnTime, test_time = time)
```

# `filter()` for relevant data 

Another typical step in data processing is to filter for the data we are interested in. For example, this dataset collected responses across four test sessions, but suppose we only want to analyse data from session one. We can create a reduced version of dataset for this purpose using the `filter()` function.

```{r filter-obs1}
filter_dat <- filter(raw_dat, time == 1)
```

Just as with the earlier functions, the first argument is the name of the dataframe that we want to filter (`raw_dat`). We then specify the conditions for retaining data: here that we want the `session` variable to be equal to (`==`) the value of 1. You can see in the environment menu that this `filter_dat` dataframe should have appeared. We've kept all `r ncol(raw_dat)` variables as we haven't instructed otherwise, but we have approximately quarter of the number of observations now. If you inspect this dataframe by clicking the grid icon, you should see that we have dropped all of the trials from sessions 2-4.

We can use different [logical operators](https://bookdown.org/rwnahhas/IntroToR/logical.html) to specify conditions in different ways.

::: {.callout-tip title="EXERCISE 3"}
What do you think the following lines of code might do to the data? Then try running them to see if your predictions are correct.
:::

```{r filter-exercise}
# Example 1
filter_dat1 <- filter(raw_dat, RT > 500)

# Example 2
filter_dat2 <- filter(raw_dat, RT <= 2000)

# Example 3
filter_dat3 <- filter(raw_dat, !is.na(RT))

# Example 4
filter_dat4 <- filter(raw_dat, item == "agouti")

# Example 5 
filter_dat5 <- filter(raw_dat, time == 1 & RT <= 2000 )

# Example 6
filter_dat6 <- filter(raw_dat, item == "banyan" | item == "caracal")
```

```{r exercise3-solution}
#| code-fold: true
#| code-summary: "Show solution"

# Example 1 - includes only responses slower than 500 ms (excludes fast responses)
filter_dat1 <- filter(raw_dat, RT > 500)

# Example 2 - includes only responses less than or equal to 2000 ms (excludes slow)
filter_dat2 <- filter(raw_dat, RT <= 2000)

# Example 3 - excludes observations where RT is missing data
filter_dat3 <- filter(raw_dat, !is.na(RT))

# Example 4 - keep only responses to "agouti" item
filter_dat4 <- filter(raw_dat, item == "agouti")

# Example 5  - keep responses from time 1 that are less than or equal to 2000 ms (excludes slow)
filter_dat5 <- filter(raw_dat, time == 1 & RT <= 2000 )

# Example 6 - keep only responses to "banyan" or "caracal" items
filter_dat6 <- filter(raw_dat, item == "banyan" | item == "caracal")

```

You can specify and combine your selection criteria in pretty much any way imaginable. Again, the key benefit here is that we haven't lost our original data at any point. Realise you've done something wrong? Edit and run it again. Need versions of your dataset with and without certain participants included? Create two versions. Reviewers query the impact of your exclusion criteria? Show them a version without. These data came from the same stored dataset (no deleting data you might regret!), and you've documented in code exactly what you've done with them. Having these steps scripted is a major win for reproducibility.

# Combining data cleaning steps using the pipe `|>` operator

So far, we've selected, renamed, and filtered our data in separate steps, reassigning the result to a new dataframe each time. This is fine, and very helpful when you are first developing your code as it allows you to inspect/test each step as you go and check things are running as expected.

However, in the long run, this can get a bit messy as we end up with lots of different dataframes in the environment. It also becomes increasingly difficult to name them all something sensible, and it's easy to lose track and make mistakes of which dataframe we are suppose to be using.

To counter this problem, the *tidyverse* has a handy function called the *pipe* operator. It looks like this: `|>` , or alternatively like this `%>%` (older version). You can read the pipe operator as "*and then*". It takes the output from the code to the left of it, *and then* sends it to the first argument of your next line of code. Remember that the first argument of the `select()`, `rename()`, and `filter()` functions was the data, so this works very nicely.

For example, we can select our variables of interest then filter for the observations we want, all in a single step as follows:

```{r pipe-example}
select_filter_dat <- raw_dat |>
  select(ID, time, item, RT) |> 
  rename(test_time = time) |> 
  filter(RT < 3000)
```

So you can read the above chunk as follows: create a new dataframe called `select_filter_dat` by taking the `raw_dat` dataframe **AND THEN** selecting relevant variables (ID, test, item, RT) **AND THEN** renaming test to test_time **AND THEN** filtering for only those trials where the RT was less than 3000 ms. We haven't needed to rename the dataframe at each stage, as the pipe has filled this in for us from the previous set.
